Algorithm: A step-by-step procedure for solving a problem or accomplishing a task
Algorithmic Thinking: The process of approaching problems systematically by breaking them down and developing step-by-step solutions
Problem Decomposition: Breaking a complex problem into smaller, more manageable subproblems
Time Complexity: A measure of how an algorithm's runtime increases as the input size grows
Space Complexity: A measure of how much memory an algorithm requires as the input size grows
Big O Notation: A mathematical notation that describes the limiting behavior of a function when the argument tends towards a particular value or infinity
Iteration: Repeating a process to generate a sequence of outcomes
Recursion: A method where the solution depends on solutions to smaller instances of the same problem
Brute Force: Solving a problem by trying all possible solutions
Optimization: The process of modifying an algorithm to improve its efficiency or performance

Measuring Algorithm Efficiency:
"There are several ways to measure algorithm efficiency:"
Execution Time: Measuring actual time taken
Simple but dependent on hardware, implementation, and input data
Useful for practical comparisons
Operation Counting: Counting fundamental operations
More abstract but gives clearer insight into algorithm behavior
Helps identify bottlenecks
Asymptotic Analysis: Analyzing growth patterns
Focuses on how algorithms scale with input size
Uses notations like Big O, Big Omega, and Big Theta


Intro to Big O Notation:
Common Big O Complexities:
O(1) - Constant Time
Runtime doesn't depend on input size
Example: Accessing an array element by index

O(log n) - Logarithmic Time
Runtime grows logarithmically with input size
Example: Binary search in a sorted array

O(n) - Linear Time
Runtime grows linearly with input size
Example: Scanning through an array once

O(n log n) - Linearithmic Time
Often seen in efficient sorting algorithms
Examples: Merge sort, quick sort (average case)

O(n²) - Quadratic Time
Runtime grows with the square of the input size
Examples: Simple nested loops, bubble sort

O(2^n) - Exponential Time
Runtime doubles with each additional input element
Example: Recursive calculation of Fibonacci numbers without memoization

O(n!) - Factorial Time
Extremely slow growth rate
Example: Brute force solution to traveling salesman problem





a brute force solution is a straightfoward, naive way to solve a problem by trying all possible options without using any shortcuts or optimizations.
key points, it checks every possible case until it finds the answer. often easy to write and understand. usually inefficient for large inputs bc it can be slow.
time complex is often high(like 0(n2), 0(2n))


When analyzing your algorithms, ask yourself these questions:"
What is the best-case scenario?
The input that causes the algorithm to finish most quickly
What is the worst-case scenario?
The input that causes the algorithm to take the longest
What is the average-case scenario?
The expected performance over all possible inputs
What happens as the input size grows?
How does performance scale with larger inputs?


"Examples of different space complexities:"
O(1) - Constant Space
Memory usage doesn't depend on input size
Example: Finding the maximum value in an array (just one variable needed)
O(n) - Linear Space
Memory usage grows linearly with input size
Example: Creating a new array with the same size as the input
O(n²) - Quadratic Space
Memory usage grows with the square of the input size
Example: Creating a 2D matrix based on input size
"Sometimes there are trade-offs between time and space complexity. An algorithm might run faster by using more memory, or save memory by taking longer to execute."




1. Bubble Sort
How it works:

Repeatedly compares adjacent elements in the list.

Swaps them if they are in the wrong order (larger elements move towards the end, smaller towards the beginning).

This process is repeated for every element in the list.

After each full pass through the list, the largest unsorted element "bubbles" up to its correct position.

Best for: Small datasets, as it can be slow for larger ones.

Time Complexity:

Worst-case: O(n²)

Best-case: O(n) (if the list is already sorted)

Average-case: O(n²)



2. Insertion Sort
How it works:

Builds a sorted portion of the list from left to right.

It picks the next unsorted element and "inserts" it into its correct position in the sorted portion by shifting elements as needed.

Best for: Small datasets or nearly sorted lists, as it’s efficient when the list is almost sorted.

Time Complexity:

Worst-case: O(n²)

Best-case: O(n) (when the list is already sorted)

Average-case: O(n²)




3. Selection Sort
How it works:

Finds the smallest (or largest) element from the unsorted portion of the list.

Swaps it with the first unsorted element.

This process repeats for the rest of the list, progressively building a sorted portion.

Best for: When you want to minimize the number of swaps, as it only makes one swap per iteration.

Time Complexity:

Worst-case: O(n²)

Best-case: O(n²) (no improvement for already sorted lists)

Average-case: O(n²)

Key Differences:
Bubble Sort: Focuses on adjacent swaps to "bubble" larger elements to the end.

Insertion Sort: Inserts elements into the correct position in a growing sorted portion.

Selection Sort: Selects the smallest/largest element and swaps it into its final position.

In general, Insertion Sort tends to be more efficient for small or nearly sorted datasets, while Bubble Sort and Selection Sort are 
considered less efficient for larger datasets due to their O(n²) time complexity.







